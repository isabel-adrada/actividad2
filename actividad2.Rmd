---
title: "Actividad 2"
author: |
  Adrada Isabel, De la Peña Juan, Terán Federico, Troncoso Samuel    
  Pontificia Universidad Javeriana Cali
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
    extra_dependencies: ["booktabs", "array", "float", "longtable"]
header-includes:
  - \usepackage{multicol}
  - \usepackage{longtable}
  - \setlength{\columnsep}{1cm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

#Instalación de las libreriás requeridas
library(ggplot2)
library(dplyr)
library(GGally)
library(xtable)
library(knitr)
library(forcats)
library(tidyr)
library(modeest)
library(kableExtra)
```

\begin{multicols}{2}

\section{Resumen}
Este estudio explora las propiedades estadísticas de una población simulada de 1000 números aleatorios y sus muestras, validando el Teorema del Límite Central (TLC). Mediante el análisis de distribuciones de frecuencias, histogramas y medias muestrales

\section{Key words}
Teorema del Límite Central, Distribución muestral, Media muestral, Inferencia estadística.

\section{Introducción}

En estadística, el Teorema del Límite Central (TLC) es un pilar fundamental que justifica la aproximación normal de las medias muestrales, independientemente de la distribución de la población original [1]. Este trabajo analiza una población simulada de 1000 datos generados aleatoriamente (0-1000) y sus muestras
En esta actividad aplicamos conceptos de estadística descriptiva para analizar una población generada aleatoriamente. Calculamos medidas comola media, mediana, moda y construimos tablas defrecuencias e histogramas para estudiar su comportamiento. Además, obtuvimos muestras aleatoriaspara comparar sus promedios con el de la poblacióntotal y analizar la variabilidad entre ellas.Complementamos este trabajo con el análisis de dosartículos científicos relacionados con las distribuciones probabilísticas: una discreta, enfocada en laoptimización de recursos en redes inalámbricas, yotra continua, aplicada al modelado de sistemas decolas. Esto nos permitió conectar la teoría con aplicaciones reales, reforzando nuestra comprensión delpapel que juegan estas distribuciones en diferentescontextos de la ingeniería.

\section{Métodos}
En primer lugar, se generaron aleatoriamente 1000 números entre 1 y 100000, los cuáles representan la población objetivo del presente estudio mediante la función sample.

```{r dataframe, echo=TRUE}
poblacion <- sample(0:1000, 1000, replace = TRUE)
```

Esta población generada aleatoriamente se guardó en un archivo datos.csv a través del código presentado en el archivo Poblacion.R, por lo cuál en este documento se trabajará con el data frame datos, cargado en el presente entorno a partir del archivo csv generado.

```{r load, echo=FALSE}
library(readr)
info <- read_csv("datos.csv", col_types = cols(.default = col_integer()))
datos <- as.numeric(unlist(info))
```

El promedio de la población objetivo de los 1000 números se obtuvo utilizando la función mean sobre el vector datos generado anteriormente.
```{r promedio, echo=TRUE}
prom <- mean(datos)
```

Para generar una tabla de frecuencia de la población, se realizaron k  intérvalos, donde k es igual a 10, con un ancho de (Max - Min)/k por intérvalo. El conteo de la cantidad de datos dentro de un determinado conforma la frecuencia absoluta, se presenta además la frecuencia relativa, frecuencia absoluta acumulada y frecuencia relativa acumulada.

Por otro lado, se graficaron los datos en un histograma para determinar de manera visual el tipo de distribución probabilística de los datos analizados.

Para obtener 5 muestras sin reposición de tamaño 10 de la población de los 1000 números aleatorios se utilizó la función sample para obtener un vector con una muestra aleatoria.
```{r muestra1, echo=TRUE}
muestra1 <- sample(datos, size = 10, replace = FALSE)
```

```{r muestras, echo=FALSE}
muestra2 <- sample(datos, size = 10, replace = FALSE)
muestra3 <- sample(datos, size = 10, replace = FALSE)
muestra4 <- sample(datos, size = 10, replace = FALSE)
muestra5 <- sample(datos, size = 10, replace = FALSE)
```

Para obtener el promedio muestral de cada una de las muestras obtenidas se utilizó la misma metodología del promedio de la población con la función mean.
```{r prom1 muestra, echo=TRUE}
prom1 <- mean(muestra1)
```

```{r prom muestras, echo=F}
prom2 <- mean(muestra2)
prom3 <- mean(muestra3)
prom4 <- mean(muestra4)
prom5 <- mean(muestra5)
```

Posteriormente, se graficó un histograma con las frecuencias delos promedios de las muestras de tamaño 10 con el objetivo de evaluar la forma de la distribución.


\section{Resultados}

Utilizando el método planteado anteriormente, el promedio de la población objetivo de los 1000 números es de `r prom`. 

En la Tabla 1 se presentan las frecuencias de los datos distribuidas en 10 intérvalos, donde se observa como todas las frecuencias absolutas para cada uno se encuentran en un rango de 82 a 113, por lo cuál es posible que los datos presenten una distribución probabilista relativamente uniforme. Esto es reafirmado por la frecuencia relativa, la cuál varía entre 0.08 y 0.11.

```{r tendenia central}
med <- median(datos)
mod <- mfv(datos)
```

Las afirmaciones anteriores llevan a afirmar que la distribución no es normal, sin embargo para rectificar esta afirmación se calculó la mediana y moda, las cuáles son `r med` y `r mod` respectivamente. Esto evidencia que aunque la media y la mediana tienen valores relativamente similares, la moda difiere significativamente de ambos valores, por lo cuál se rectifica que la distribución de los datos no es normal.


\end{multicols}
```{r tabla1}
ancho <- (max(datos) - min(datos))/10

cortes <- seq(min(datos), max(datos), by = ancho)
etiquetas <- character(length(cortes)-1)
etiquetas[1] <- paste0("[", round(cortes[1], 2), ", ", round(cortes[2], 2), "]")
if(length(cortes) > 2) {
  for(i in 2:(length(cortes)-1)) {
    etiquetas[i] <- paste0("(", round(cortes[i], 2), ", ", round(cortes[i+1], 2), "]")
  }
}
intervalos <- cut(datos, breaks = cortes, labels = etiquetas, include.lowest = TRUE, right = TRUE)

frec_abs <- table(intervalos)
frec_rel <- prop.table(frec_abs)
frec_abs_acum <- cumsum(frec_abs)
frec_rel_acum <- cumsum(frec_rel)

tabla_frecuencias_commercial <- data.frame(
  Intervalo = levels(intervalos),
  "F Absoluta" = as.numeric(frec_abs),
  "F Relativa" = round(as.numeric(frec_rel), 2),
  "F Abs Acum" = as.numeric(frec_abs_acum),
  "F Rel Acum" = round(as.numeric(frec_rel_acum), 2)
)

knitr::kable(tabla_frecuencias_commercial, 
             caption = "Frecuencia de los datos",
             align = c('l', 'c', 'c', 'c', 'c'))

```
\begin{multicols}{2}

A través de la Figura 1 se presenta la distribución de los datos en un histograma para verificar visualmente la distribución propabilística de los datos. En esta se observa como los datos en los diferentes intérvalos tienen frecuencias similares, sin embargo no es una distribución exactamente uniforme debido a las diferencias en las frecuencias del histograma y se muestra una leve tendencia de disminución en la frecuencia a medida que aumenta el intérvalo.


```{r figura1}
cortes <- seq(0, 1000, by = 100)
histograma <- ggplot(data.frame(datos = datos), aes(x = datos)) +
  geom_histogram(breaks = cortes, fill = "steelblue", color = "white", boundary = 0) +
  labs(x = "Intérvalos", y = "Frecuencia") +
  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 100), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, NA) ,breaks = seq(0, 140, by = 20), expand = c(0, 0)) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())

ggsave( filename = "figura1.png", plot = histograma,
  width = 6, height = 4, dpi = 150
)
```
\begin{center}
\includegraphics[width=\linewidth]{figura1.png}
\end{center}
Figura 1. Distribución de los datos.

A continuación se presentan 5 muestras aleatorias sin reposición de tamaño n = 10:

Muestra 1: `r muestra1`

Muestra 2: `r muestra2`

Muestra 3: `r muestra3`

Muestra 4: `r muestra4`

Muestra 5: `r muestra5`

Estas muestras tienen los promedios muestrales `r prom1`, `r prom2`, `r prom3`, `r prom4` y `r prom5` respectivamente, mostrando una alta variabilidad de la media de las muestras obtenidas.

Los promedios de las muestras de tamaño 10 siguen una distribución aproximadamente normal (en forma de campana de Gauss), a pesar de que la población original no necesariamente tenga esta forma. Esto se debe al Teorema del Límite Central (TLC), que establece que, bajo condiciones generales, la distribución de las medias muestrales se aproxima a una normal a medida que aumenta el tamaño de la muestra, incluso si los datos originales no son normales.

 
```{r, out.width="100%", fig.pos="H"}
knitr::include_graphics("prom_muestrales.png")
```
Figura2: Distribución de frecuencias de la población.

El promedio de las medias muestrales resulta casi idéntico al de la población, confirmando que la media muestral es un estimador insesgado. Por otro lado, la dispersión de las medias muestrales es notablemente menor que la de la población original, lo que concuerda con la teoría estadística: la varianza de las medias disminuye proporcionalmente al tamaño de la muestra (σ²/n). Esto explica por qué muestras más grandes producen estimaciones más precisas.
Con muestras pequeñas (como n >=10), la aproximación a la normal puede no ser perfecta, pero ya se observa una tendencia hacia la simetría y concentración alrededor de la media poblacional. Si el tamaño de muestra fuera mayor (ej. n > =30), la distribución sería aún más claramente normal.

\section{Análisis de resultados}

A partir de los resultados obtenidos se puede determinar que la distribución de los datos generados aleatoriamente como población tiene una distribución prbabilística relativamente similar a la distribución uniforme.

Al obtener muestras de 10 datos de la población se encuentra que las medias de las diferentes muestras presentan una significativa variación entre sí.

Este ejercicio práctico valida tres principios clave: (1) el Teorema del Límite Central, pues las medias muestrales siguen una distribución normal aunque la población no lo haga; (2) la media muestral es un estimador robusto de la media poblacional; y (3) el muestreo reduce la variabilidad, permitiendo inferencias confiables incluso con muestras pequeñas. Estos hallazgos justifican el uso de técnicas estadísticas basadas en muestras para analizar poblaciones grandes

\section{Análisis artículo: distribución continua
}
En el artículo An Application to Continuous Probability Distribution, escrito por B. V. Dhandra y S. S. Kalyani, se presenta un enfoque práctico sobre el uso de distribuciones de probabilidad continuas para analizar el comportamiento de sistemas de colas. Este trabajo se enfoca en modelar la acumulación de personas en distintos escenarios usando funciones de densidad, destacando su aplicabilidad en contextos urbanos y rurales.

El artículo presenta una aplicación de las distribuciones de probabilidad continuas para modelar la cantidad de personas que se acumulan en un sistema de colas durante un intervalo de tiempo. A través del uso de funciones de densidad, se logra estimar con mayor precisión la probabilidad de que cierto número de personas esté presente en el sistema, lo cual es esencial para evaluar la eficiencia de distintos entornos de atención, como sucursales bancarias urbanas y rurales.

Se realizaron comparaciones entre dos sistemas de colas con diferentes tasas de llegada (λ), observando que un mayor valor de λ genera una mayor acumulación de personas, lo que indica menor eficiencia operativa. A través de esta técnica se pueden tomar decisiones basadas en datos para mejorar la asignación de recursos y la planificación de atención al cliente.

Este estudio resulta especialmente relevante para estudiantes y profesionales de Ingeniería de Sistemas, ya que proporciona herramientas matemáticas para el modelado de sistemas reales. Además, promueve el uso de distribuciones probabilísticas como base para simulaciones, análisis de rendimiento y optimización de procesos, habilidades claves en áreas como redes, ingeniería del software y sistemas inteligentes.

\section{Conclusiones}
En este estudio, se analizó una población aleatoria de 1000 datos para evaluar sus características estadísticas y distribución probabilística. Los resultados indican que la distribución es relativamente uniforme, aunque con ligeras variaciones entre los intervalos. Las medidas estadísticas, como la media, mediana y moda,4 confirmaron que los datos no presentan una distribución normal. Además, las muestras aleatorias extraídas de la población mostraron una alta variabilidad en sus medias, destacando la importancia de la representatividad muestral. Por otro lado, se revisó el artículo científico “Method for Optimal Allocation of Network Resources Based on Discrete Probability Model” de Zhengqiang Song y Guo Hao. Este trabajo propone un modelo innovador para optimizar recursos en redes inalámbricas, mejorando la eficiencia y minimizando el consumo energético mediante análisis probabilísticos y algoritmos adaptativos. Su relevancia radica en los avances que aporta al Internet de las Cosas (IoT) y las redes dinámicas.

En conclusión, tanto el análisis estadístico como la revisión del artículo destacan el valor de los modelos probabilísticos para abordar problemas complejos, desde la evaluación de datos hasta la optimización de sistemas tecnológicos.

\section{Referencias}
[1]   W. Navidi, Statistics for Engineers and Scientists w/ CD-ROM. McGraw-Hill Sci./Eng./Math, 2004.

[2] Z. Song and G. Hao, “Method for optimal allocation of network resources based on discrete probability.

[3] B. V. Dhandra and S. S. Kalyani, "An Application to Continuous Probability Distribution," Jul. 2023. model,” Wireless Networks, 2022. DOI: 10.1007/s11276-021-02727-7.
\end{multicols}
